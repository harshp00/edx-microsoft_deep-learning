{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Deep Learning Explained\n\n# Module 3 - Lab - Introduction to Deep Neural Networks \n\n## 1.0 Overview\n\nThis lesson introduces you to the basics of neural network architecture in the form of deep forward networks. This architecture is the quintessential deep neural net architecture. In this lab you will learn the following:\n\n- Why is deep learning important and how it relates to representation, learning and inference.\n- How a basic preceptron works.\n- How to apply different types of loss functions. \n- Understand why nonlinear activation is important and why rectified linear units are a good choice.\n- How back propagation works, and how you apply the chain rule of calculus to determine gradient. \n- Understand the architectural trade-off between depth and width in deep networks."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 1.1 Why is deep learning important?\n\nDeep learning methods are a form of **artificial intelligence (AI)** or **machine intelligence**. More specifically, deep learning algorithms are a type of **machine learning**. \n\nWhat properties does machine intelligence require? There have been many answers to this question over the history of computing. In this case, we will take a practical view, sometimes known as **weak AI**. There are three key properties an intelligent machine must have; a **representation** for the AI model, **inference** for the output of the model, and **learning** to train the model. The figure below shows a highly abstracted view of machine intelligence, showing the relationship between representation, learning and inference. \n\n<img src=\"img/MachineIntelligence.jpg\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n\n<center>**Schematic view of machine intelligence**</center>\n\n**That's it!** The entire rest of this course will focus on just these three points: representation, learning and inference!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2.0 Forward propagation: The representation problem\n\nTo create useful neutral network we need a **representation** that has two important properties.   \n\nFirst, there needs to be a way to represent complex functions of the input. Without this property, nothing is gained, since there are numerous machine learning algorithms that work with simple representations. We will spend the rest of this section exploring this problem.   \n\nSecond, the representation needs to be **learnable**. Quite obviously, no machine intelligence representation is useful if there is not a practical algorithm to learn it. We will take up this problem in another section. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.1 Linear networks\n\nLet's start with the simplest possible network. It has inputs, and an output. The output is a **afine transformation** of the input values. We say this network performs an afine transformation since there is a bias term $b$. \n\n\n<img src=\"img/LinearNetwork.jpg\" alt=\"Drawing\" style=\"width:400px; height:250px\"/>\n\n<center>**Figure 2.1**\n**A simple afine network**</center>\n\nThis output $y$ of this network is just:\n\n$$y = f(x) = \\sum_i w_i \\cdot x_i + b$$\n\nThis network performs linear regression. Being able to perform only afine transformations, it can't do anything else. \n\nThis representation is certainly learnable. However, it does not gain us anything over familiar linear regression methods. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.2 The preceptron"
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "To get started, let's have a look at a simple **preceptron** model. The perceptron was proposed by Rosenblatt (1962). He built on the earlier attempts at a neural network models by McCulloch and Pitts (1943) and Heeb (1949). The perceptron adds **nonliner activation** to the afine network. \n\n\n<img src=\"img/Preceptron.jpg\" alt=\"Drawing\" style=\"width:350px; height:250px\"/>\n<center>**Figure 2.2 Schematic of perceptron with nonlinear activation**</center>\n\nThe output $y$ of the perceptron is given by the following:\n\n$$y = f(x) = \\sigma \\Big( \\sum_i w_i \\cdot x_i + b \\Big)$$\n\nThe output of the network is now nonlinear, give the **activation function** $\\sigma(x)$. \n\nBut, the preceptron is nothing more than a logistic regression classifier. The fact that the preceptron could only solve linearly separable problems was pointed out by Minsky and Papert (1969). The failure of the preceptron to learn an **exclusive or (XOR)** function is well known. See for example, Section 6.1 in GBC. \n\nAgain, this representation is certainly learnable. However, as before, it does not gain us anything over well known logistic regression models."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.3 Forward networks - A better representation!\n\nThe problem with the  perceptron is one of representation. There is no way that this simple network can represent anything but a linearly separable function. To represent more complex functions, we need a more complex network. In more technical terms we need a network with greater **model capacity**. \n\nWhat we need is a network with layers of **hidden nodes** with nonlinear activation. The figure below shows a simple example of a neural network with one **hidden layer** with two nodes. Since every node (including inputs) is connected to every other node we call this architecture a **fully connected neural network**.\n\n\n<img src=\"img/Hidden.jpg\" alt=\"Drawing\" style=\"width:600px; height:350px\"/>\n<center>**Figure 2.3  \nFully connected neural network with single hidden layer**</center>\n\nLet's walk through some aspects of these diagrams. \n\n1. The neural network is divided into three layers. The input layer, the hidden layer and the output layer. \n2. The values in the input layer are multiplied by a weight matrix, $W^1$.\n3. The nodes in the hidden layer sum their inputs and add a bias term, $b^1$. \n4. The outputs of the hidden layer nodes are multiplied by a weight vector, $W^2$.\n5. The output layer sums the inputs and adds another bias term, $b^2$."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.4 Neural network architectures - Finding representations\n\nThe representations achievable by neural network with just a single hidden layer  are quite powerful. In fact, Cybenko (1989) showed that such a network with an infinite number of hidden units using sigmoidal activation can approximate any arbitrary function. Hornik (1991) generalized this to apply to any activation function. We call this theorem the **universal approximation theorem**.  \n\nA universal approximation theorem may seem like a really exciting development; especially if you are a machine intelligence nerd. However, one must be circumspect when viewing such a result. A representation with an infinite number of nodes cannot be learned in any practical sense. Still it is comforting to know that, at least in principle, a representation can be learned for arbitrarily complex problems. \n\nWhile infinitely wide networks with a single layer are unrealistic, we are not limited to one layer. In fact, depth is typically more effective at creating complex representations rather than width in neural networks. Depth is measured by the count of hidden layers stacked one on top of the other in the network. Hence, the term deep neural networks. \n\nThe Figure 2.4 below shows the results of an empirical study by Goodfellow, Shlens and Szegedy (2014) of accuracy of the network vs depth. Notice that accuracy increases rapidly with depth until about 8 layers, after which the effect is reduced. \n\n\n<img src=\"img/Accuracy-Layers.jpg\" alt=\"Drawing\" style=\"width:600px; height:350px\"/>\n<center>**Figure 2.4 Empirical results of accuracy vs. number of layers**  \nDiagram from Goodfellow et. al. 2014</center>\n\nAnother view of the empirical study by Goodfellow et. al. is shown in Figure 2.5 below. In this case accuracy verses number of model parameters is compared for three different network architectures. The deeper network (11 layers) makes more  efficient use of the parameters in terms of improved accuracy. The number of parameters in a layer is approximately the total number of parameters divided by the number of layers. Notice that for the particular case tested convolutional neural networks are more efficient than fully-connected networks. We will discuss convolutional neural networks in a subsequent lesson. \n\nOf particular interest is the fact that the fully-connected network and the shallow convolutional neural network appear to be over-fitting as the test accuracy actually decreases as the number of parameters increases. We will discuss the significant problems of over-fitting in neural networks in a subsequent lesson. \n\n\n<img src=\"img/Accuracy-Parameters.jpg\" alt=\"Drawing\" style=\"width:600px; height:350px\"/>\n<center>**Figure 2.5 Empirical results of accuracy for different network architectures**  \nDiagram from Goodfellow et. al. 2014</center>\n\n**Summary:** Deep networks tend to produce better models, with less tendency to over-fit, for a given level of complexity. "
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "### 2.6 Activation functions\n\nWithout a nonlinear activation function, a neural net is just an afine transformation. Afine transformations limit representation to only linearly separable functions. To create more general representations **nonlinear activation functions** are required. \n\nIn present practice, four types of activation functions are generally used for fully connected networks. \n\n1. **Linear** activation is used for the output layer of regression neural networks. \n2. The **rectilinear** activation function is used for most hidden units. The rectilinear activation function is often referred to as **ReLU**.\n3. A **leaky rectilinear** activation acts like a ReLU function for positive inputs, but has a small negative value or leakage for negative input values. The leaky ReLU activation function can improve training for some deep neural networks, as the derivative is not zero below 0. \n3. The **logistic** or **sigmoid** activation function is used for binary classifiers.\n4. The **softmax** activation function is used for multi-class classifiers. \n\nRectilinear functions are typically used as the activation function for hidden units in neural networks. The rectilinear function is defined at:\n\n$$f(x) = max(0, x)$$\n\nThe rectilinear function is linear for positive responses and zero for responses less than 0.0. Notice that the derivatives of the rectilinear function are not continuous. While this might seem to be a problem, in practice, even gradient-based optimization functions work well with this activation function. \n\nThe rectilinear function is plotted in the cell below:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n## Compute the lectilinear function\ndef reclu(x): return(max(0,x))\nx = [float(x)/100 for x in range(-100, 100)]\ny = [reclu(y) for y in x]\n## Plot the result\ndef plot_figs(x,y,title, figsize = (4, 3)):\n    plt.figure(figsize=figsize).gca() # define axis\n    sns.set_style(\"darkgrid\")\n    plt.plot(x, y)\n    plt.ylim((-0.1,1.0))\n    plt.title(title)\n    plt.xlabel('X')\n    plt.ylabel('Y')\nplot_figs(x,y,'The Rectilinear Function')    ",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/matplotlib/font_manager.py:281: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n  'Matplotlib is building the font cache using fc-list. '\n",
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<matplotlib.figure.Figure at 0x7fc04337fcf8>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Another widely used activation function is the **logistic** or **sigmoid**. The sigmoid is used as the activation for the output layer of a binary classifier. The general sigmoid function can be written as:\n\n$$\\sigma(x) = \\frac{L}{1 + e^{-k(x_0-x)}}\\\\\nwhere\\\\\nL = max\\ value\\\\\nk = slope\\\\\nx_0 = sigmoid\\ midpoint$$\n\nWith $L=1$, $k=1$, and $x_0 = 0$, the logistic function becomes: \n\n$$\\sigma(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{1+e^x}$$\n\nThe sigmoid function can asymptotically approach $0$ or $1$, but will never reach these extreme values. However, because of the rapid decrease in the derivative away from $0$ the sigmoid can **saturate** when using gradient-based training. For this reason, the sigmoid is typically not used for hidden layers in neural networks.   \n\nWhen used in a binary classifier a threshold is set to determine if the result is $0$ or $1$. The threshold can be adjusted to bias the result as desired. \n\nThe code in the cell below plots the sigmoid function. "
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from math import exp\ndef sigmoid(x): return exp(x)/(1 + exp(x))\nx = [float(x)/100 for x in range(-700, 700)]\ny = [sigmoid(y) for y in x]\nplot_figs(x,y,'The Logistic Function', figsize = (5,3))    ",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<matplotlib.figure.Figure at 0x7fc0433e4518>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "The **softmax** function or **normalized exponential function** is used for the output activation function of a multi-class classifiers. The softmax function is the multinomial generalization of the sigmoid or logistic function. The probability of each class $j$ is written as: \n\n$$\\sigma(z_j) = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}$$\n\nThe normalization $\\sum_{k=1}^K e^{z_k}$ ensures the sum of probabilities for all classes add to $1.0$. The class selected by the classifier is the class with the largest value of $\\sigma(z_j)$."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.7 Computational example\n\nNow that we have gone though some basic theory for feed-forward networks, let's try a simple example. We will construct a fully connected network to compute this simple function:\n\n$$y = x_1 - x_2$$\n\n****\n**Comment.** You have likely noticed that this function is linear and can be computed easily without a neural network. Of course, that is not the point. We use a simple function to make the results easy to understand. \n****\n\n****\n**Note.** The neural network for this example does not require any bias terms. \n****\n\nAs a first step, we will create test data for 3 cases; $x_1 > x_2$, $x_1 = x_2$, and $x_1 <x_2$"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = [(2,1), (1,1), (1,2)]\nfor x_in in x:\n    print(x_in[0] - x_in[1])",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "1\n0\n-1\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now that we have our test data and test cases we can move to the next step. We will create the first weight tensor."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nW_1 = np.array([[1.0, -1.0], [-1.0, 1.0]])\nprint(W_1)",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[ 1. -1.]\n [-1.  1.]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We also need the weight tensor, for the second set of weights. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "W_2 = np.array([1, -1])\nprint(W_2)",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[ 1 -1]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We have the input test cases and the network weights. Now, it is time to compute the results and check them. The code in the cell below performs the feed forward network computation. The first function computes the matrix product of the weights with the input tensor and applies a rectilinear activation function. This function computes the output of the hidden layer given the input tensor. \n\n$$h = \\sigma(W^1 \\cdot x)$$\n\nThe second function computes the vector product of the weight tensor with the output tensor of the hidden layer which is the output of the network: \n\n$$o = W^2 \\cdot h$$"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def hidden(x, W):\n    \"\"\"Computes the output of the hidden layer\"\"\"\n    h = np.dot(W, x) # product of weights and input vector\n    return np.array([reclu(x) for x in h]) # apply activation function and return\n\ndef output(h, W):\n    \"\"\"Computes the result for the hidden layer\"\"\"\n    return np.dot(W, h) # dot product of weight vector and input vector\n    \n## Run the test cases and check the results     \nfor y in x:\n        h = hidden(y, W_1)\n        print(output(h, W_2))",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "1.0\n0\n-1.0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our first fully connected neural network passes all the tests!\n\nNotice that even a network to compute a simple function requires 6 weights. You can see that for more complex functions any practical algorithm must learn a large number of weights. The limitations of Numpy would quickly become evident for large scale problems involving hundreds of millions of weights. \n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "*******\n**Exercise 1:** The first simple neural network worked as intended. But, what happens if you change the activation of the hidden units? In the cell below, modify the code from the previous example to use sigmodial activation. Use an expanded set of test cases `[(3,1), (2,1), (1,1), (1,2), (1,3)]`. \n*******"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = [(3,1), (2,1), (1,1), (1,2), (1,3)]\ndef hidden(x, W):\n    \"\"\"Computes the output of the hidden layer\"\"\"\n    h = np.dot(W, x) # product of weights and input vector\n    return np.array([sigmoid(x) for x in h]) # apply activation function and return\n\ndef output(h, W):\n    \"\"\"Computes the result for the hidden layer\"\"\"\n    return np.dot(W, h) # dot product of weight vector and input vector\n    \n## Run the test cases and check the results     \nfor y in x:\n        h = hidden(y, W_1)\n        print(output(h, W_2))\n",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0.7615941559557649\n0.4621171572600098\n0.0\n-0.4621171572600098\n-0.7615941559557649\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "How have the output values changed? Did you need to update the output weight tensor values to get a better approximation of the function? "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3.0 Learning in neural networks: Backpropagation\n\nNow that we have a promising representation, we need to determine if it is trainable. The answer is not only yes we can, but that we can do so in a computationally efficient manner, using a cleaver algorithm known as **backpropagation**. \n\nThe backpropagation algorithm was developed independently multiple times. The earliest work on this algorithm was by Kelly (1960) in the context of control theory and Bryson (1961) in the context of dynamic programming. Rumelhart, Hinton and Williams (1986) demonstrated empirically that backpropagation can be used to train neural networks. Their paper marks the start of the modern history of neural networks, and set off the first wave of enthusiasm. \n\nThe backpropagation algorithm requires several components. First, we need a **loss function** to measure how well our representation matches the function we are trying to learn. Second, we need a way to propagate changes in the representation through the complex network For this we will use the **chain rule of calculus** to compute **gradients** of the representation. In the general case, this process requires using automatic differentiation methods. \n\nThe point of backpropagration is to learn the optimal weight for the neural network. The algorithm proceeds iteratively through a series of small steps. Once we have the gradient of the loss function we can update the tensor of weights.\n\n$$W_{t+1} = W_t + \\alpha \\nabla_{W} J(W_t) $$  \nwhere  \n$W_t = $ the tensor of weights or model parameters at step $t$.   \n$\\alpha\\ = $ step size or learning rate.  \n$J(W) = $ loss function given the weights.  \n$\\nabla_{W} J(W) = $ gradient of $J$ with respect to the weights $W$.  \n\nIt should be evident that the back propagation algorithm is a form of gradient decent. The weights are updated in small steps following the gradient of $J(W)$ down hill. \n\nFinally, we need a way evaluate the performance of the model. Without evaluation metrics we have no way to compare the performance of a given model, or compare the performance of several models. \n\nIn the next sections, we will address each of loss functions, gradient computation and performance measurement. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.1 Loss functions\n\nTo train a neural network we must have a **loss function**, also known as a **cost function**. In simple terms, the loss function measures the fit of a model to the training data. The lower the loss, the better the fit. \n\nTo train deep learning models **cross entropy** is often used as a loss function. This is an information theoretic measure of model fit. We can understand cross entropy as follows. \n\nFirst define **Shannon entropy** as:\n\n$$\\mathbb{H}(I) = E[I(X)] = E[-ln_b(P(X))] = - \\sum_{i=1}^n P(x_i) ln_b(P(x_i)$$  \nWhere:  \n$E[X] = $ the expectation of $X$.  \n$I(X) = $ the information content of $X$.   \n$P(X) = $ probability of $X$.  \n$b = $ base of the logarithm.    \n\nThis rather abstract formula gives us a way to compute the expected information content of a set of values $X$. The more likely (higher probability) of $X$ the less informative it is. \n\nTo create a loss function from the definition of Shannon entropy we start with the **Kullback-Leibler divergence (KL divergence)** or **relative entropy**. The KL divergence is an information theoretic measure of the difference between two distributions, $P(X)$ and $Q(X)$.\n\n$$\\mathbb{D}_{KL}(P \\parallel Q) = - \\sum_{i=1}^n p(x_i)\\ ln_b \\frac{p(x_i)}{q(x_i)}$$\n\nIdeally, in the case of training a machine learning model we want a distribution $Q(X)$, which is identical to the actual data distribution $P(X)$. \n\nBut, you may say, if we could know $P(X)$ why compute $Q(X)$ at all? Fortunately, we do not have to. We can rewrite the KL divergence as:\n\n$$\\mathbb{D}_{KL}(P \\parallel Q) = \\sum_{i=1}^n p(x_i)\\ ln_b p(x_i) - \\sum_{i=1}^n p(x_i)\\ ln_b q(x_i)$$\n\nSince $P(X)$ is fixed and we wish to find $Q(X)$ when we train our model, we can minimize the term on the right, which is the **cross entropy** defined as:\n\n$$\\mathbb{H}(P,Q) = - \\sum_{i=1}^n p(x_i)\\ ln_b q(x_i)$$\n\nFrom the formulation of KL divergence above you can see the following.\n\n$$\\mathbb{D}_{KL}(P \\parallel Q) = \\mathbb{H}(P) + \\mathbb{H}(P,Q)\\\\\n\\mathbb{D}_{KL}(P \\parallel Q) = Entropy(P) + Cross\\ Entropy(P,Q)$$\n\nThus, we can minimize divergence by minimizing cross entropy. This idea is both intuitive and computationally attractive. The closer the estimated distribution $q(X)$ is to the distribution of the true underling process $p(X)$, the lower the cross entropy and the lower the KL divergence. \n\nIn general we will not know $p(X)$. In fact, if we did, why would we need to solve a training problem? So, we can use the following approximation.\n\n$$\\mathbb{H}(P,Q) = - \\frac{1}{N} \\sum_{i=1}^n ln_b q(x_i)$$\n\nYou may notice, that this approximation, using the average log likelihood, is equivalent to a maximum likelihood estimator (MLE). "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.2 Cross Entropy for Gaussian Likelihood\n\nLet's look at a specific case of a model with Gaussian likelihood. What is the cross entropy? We can start by thinking about the definition of likelihood. \n\n$$p(data|model) = p(data|f(\\theta)) = p(x_i|f(\\hat{\\mu},\\sigma))= \\frac{1}{2 \\pi \\sigma^2} e^{\\frac{-(x_i - \\hat{\\mu})^2}{2 \\sigma^2}}$$\n\nWe take the negative logarithm of this likelihood model. \n\n$$-log\\big(p(data|model) \\big) = - \\frac{1}{2}\\big( log( 2 \\pi \\sigma^2) + \\frac{(x_i - \\hat{\\mu})^2}{2 \\sigma^2} \\big)$$\n\nNow, the first term on the right is a constant, as is the denominator of the second term if we assume known variance. Since our goal is to minimize cross entropy, we can eliminate these quantities and be left with just the following.\n\n$$-(x_i - \\hat{\\mu})^2$$\n\nThis is one issue we need to deal with. Our formulation of cross entropy involves the unknown true distribution of the underling process $p(X)$. However, since $p(x_i)$ is fixed but unknown we can just write the following.\n\n$$min \\big( \\mathbb{H}(P,Q) \\big) \\propto argmin_{\\mu} \\big( - \\sum_{i=1}^n (x_i - \\hat{\\mu})^2 \\big)$$\n\nThis is just the definition of a Maximum Likelihood Estimator (MLE) for the least squares problem! In fact, since the cross entropy is computed using the negative log likelihood, it will always be minimized by the MLE. \n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.3 Chain rule of calculus\n\nKey to the back propagation algorithm is the chain rule of calculus; not to be confused with the chain rule of probability. The chain rule allows us to back propagate gradients though an arbitrarily complex graph of functions. \n\nNow, suppose there is a function $y = g(x)$, and another function $z = f(y) = f(g(x))$. How do we compute the derivative of $z$ with respect to $x$? Applying the chain rule we get: \n\n$$\\frac{dz}{dx} = \\frac{dz}{dy}\\frac{dy}{dx}$$\n\nConsider $x \\in R^M$ $g(x) \\Rightarrow R^M$ and $ f(y) \\Rightarrow z \\in R$. The chain rule becomes:\n\n$$\\frac{\\partial z}{\\partial x} = \\sum_{j \\in M} \\frac{\\partial z}{\\partial y_j}\\frac{\\partial y_j}{\\partial x_i}$$\n\nWhich we can rewrite as  \n\n$$\\nabla_{x}z = \\Big( \\frac{\\partial x}{\\partial y} \\Big)^T \\nabla_{y}z$$  \n\nHere, $\\frac{\\partial x}{\\partial y}$ is the $n x m$ **Jacobian matrix** of partial derivatives. The Jacobian is multiplied by the gradient with respect to $y$, $\\nabla_{y}z$. You can think of the Jacobian as a transformation for a gradient with respect to $y$ to what we really want, the gradient with respect to $z$."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.4 Example of finding a gradient. \n\nLet's work out backpropagation for a very simple neural network with a single hidden layer with two units. This neural network, including the loss function, is shown in Figure 3.1 below. This network has been highly simplified. There are only three layers, input layer, a two unit hidden layer with no bias terms, and a single unit output layer. There are only two weight tensors for this network. Further, the hidden units use rectilinear activation and the output unit uses linear activation. These activation functions have simple partial derivatives.  \n\n<img src=\"img/LossGraph.jpg\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n<center>**Figure 3.1 \nSimple single layer neural network with loss function** </center>\n\nFirst, we need to work out the forward propagation relationships. We can compute the outputs of the hidden layer as follows.\n\n$$S_{\\{1,2\\}} = \\sigma_h \\big( W^1 \\cdot X_{\\{1,2\\}} \\big) = \\sigma \\big( \\sum_j W^1_{i,j} x_j \\big)$$  \n\nIn the same way, the result from the output layer can be computed as follows, since the activation function for this layer is linear. \n\n$$S_3 = W^2 \\cdot S_{\\{1,2\\}} = \\sum_i W^2_i \\sigma \\big( \\sum_j W^1_{i,j} x_j \\big)$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To perform backpropagation, we need fill out the gradient vector by computing $\\frac{\\partial J(W)}{\\partial W}$ for each weight in the model. \n\n$$\\frac{\\partial J(W)}{\\partial W} = \n\\begin{bmatrix} \n\\frac{\\partial J(W)}{\\partial W^2_{11}} \\\\\n\\frac{\\partial J(W)}{\\partial W^2_{12}} \\\\\n\\frac{\\partial J(W)}{\\partial W^2_{21}} \\\\\n\\frac{\\partial J(W)}{\\partial W^2_{22}} \\\\\n\\frac{\\partial J(W)}{\\partial W^1_{1}} \\\\\n\\frac{\\partial J(W)}{\\partial W^1_{2}}\n\\end{bmatrix}$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To keep things simple in this example we will just use a non-normalized squared error loss function. This is just the MLE estimator (without normalization) for a Gaussian distribution. \n\n$$J(W) = - \\frac{1}{2} \\sum_{l=1}^n (y_l - S_{3,l})^2 $$\n\nWhere:  \n$y_k = $ the label for the lth case.     \n$\\hat{y_k} = S_{3,k} =$ the output of the network for the lth case. \n\nWe want to compute the gradients with respect to the input and output tensors:\n\n$$\\frac{\\partial J(W)}{\\partial W^1}, \\ \\frac{\\partial J(W)}{\\partial W^2}$$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's start with the easier case of the partial derivatives with respect to the output tensor. We can apply the chain rule as follows:\n\n$$\\frac{\\partial J(W)}{\\partial W^2_k} = \\frac{\\partial  J(W)}{\\partial S_{3,k}} \\frac{\\partial S_{3,k}}{\\partial W^2_k}$$\n\nThe first partial derivative of the chain is:\n\n$$\\frac{\\partial J(W)}{\\partial S_{3,k}} = \\frac{\\partial - \\frac{1}{2} (y_k - S_{3,k})^2} {\\partial S_{3,k}} = y_k - S_{3,k} $$\n\nAnd, the second partial derivative in the chain, given the linear activation of the output unit, becomes:\n\n$$\\frac{\\partial S_{3,k}}{\\partial W^2_k} = \\frac{\\partial W^2_k S_{j,k}}{\\partial W^2_k}  = S_{j,l}, \\ j \\in \\{1,2\\}$$\n\nMultiplying the two components of the chain gives us:\n\n$$\\frac{\\partial J(W)}{\\partial W^2_k} = S_{j,k} (y_k - S_{3,k}), \\ j \\in \\{1,2\\} $$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The partial derivatives with respect to the input tensor are a bit more complicated. To apply the chain rule we must work backwards from the loss function. This gives the following chain:\n\n$$\\frac{\\partial J(W)}{\\partial W^1_{i,j}} =  \\frac{\\partial J(W)}{\\partial S_{3}} \\frac{\\partial S_{3}}{\\partial S_{j}} \\frac{\\partial S_{j}}{\\partial W^1_{i,j}}$$\n\nFirst, we find the right most partial derivative in our chain:\n\n\\begin{equation}\n\\frac{\\partial S_j}{\\partial W^1_{i,j}} = \n\\begin{cases}\n     \\frac {\\partial W^1_{i,j} x_{i,k}}{\\partial W^1_{i,j}}, & \\text{if $S_j>0$} \\\\\n    0, & \\text{otherwise}\n  \\end{cases}\n\\end{equation}\n\nWhich given the ReLU activation results in:\n\n\\begin{equation}\n\\frac{\\partial S_j}{\\partial W^1_{i,j}} = \n\\begin{cases}\n    1, & \\text{if $S_j>0$}  \\\\\n    0, & \\text{otherwise}\n  \\end{cases}\n\\end{equation}\n\n\nThe middle partial derivative must account for the nonlinearity:\n\n$$\\frac{\\partial S_{3}}{\\partial S_{j}} = W^2_j$$\n\nWe have already computed $\\frac{\\partial J(W)}{\\partial S_{3}}$. Multiplying all three partial derivatives we find:\n\n\\begin{equation}\n\\frac{\\partial J(W)}{\\partial W^1_{i,j}} = \n\\begin{cases}\n    (y_k - S_{3,k}) W^2_j, & \\text{if $S_j>0$} \\\\\n    0, & \\text{otherwise}\n  \\end{cases}\n\\end{equation}\n\nWhere $S_3$ and $S_{\\{1,2 \\}} are computed using the relationships given above. \n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4.0 Network for XOR Function\n\nIn an earlier section the work of Minsky and Papert was mentioned. A key failure of the early neural network architectures, such as the perceptron, was the inability to learn function that are not linearly separable. The **exclusive OR** or **XOR** is just such a function. The truth table of the XOR looks like this:\n\n| Input 1 | Input 2 | Output |\n|---------|---------|--------|\n| 0 | 0 | 0|\n| 0 | 1 | 1 |\n| 1 | 0 | 1 |\n| 1 | 1 | 0 |\n\nIn words, the XOR function is 0 if both inputs are the same, or 1 if both inputs are different. Hence, the reason it is known as exclusive OR. \n\n****************************\n**Exercise 2:** In the cell below, you will create code for a neural network which performs the XOR operation. The following elements are required:\n\n1. An input weight tensor\n2. A hidden layer with two units using ReLU activation\n3. A output weight tensor\n4. An output unit with linear activation\n\nThere are 4 possible test (input) cases. Test your code for all cases.  "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nW_1 = np.array([[1.0, -1.0], [-1.0, 1.0]])\nprint(W_1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## 5.0 Performance Metrics\n\nNow that we have the components for training a basic neural network in place we need a way to evaluate the performance. It turns out, there is nothing special evaluation of neural network models as opposed to other machine learning models. \n\nFor regression models, one typically uses the standard metrics such as root mean square error (RMSE), mean absolute error (MAE). \n\nFor classification models, one also typically uses the standard metrics including the confusion matrix, accuracy, precision and recall. "
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}